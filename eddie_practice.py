# -*- coding: utf-8 -*-
"""eddie_practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v8Y7Onu35dm4UD7Ub9BQnGBzEeumv4Gc
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
import time
from datetime import date, timedelta

news_dates = []
start_date = date(2019, 5, 29)
end_date = date(2019, 7, 28)
delta = timedelta(days=1)

while start_date <= end_date:
  newsdate = start_date.strftime("%Y-%m-%d")
  news_dates.append(newsdate)
  start_date += delta

import pandas as pd

path = r'/content/drive/My Drive/g20_uplevel/jackie_json/'
json_end = r'.json'

master_df = pd.read_json(str(path + str(news_dates[0]) + json_end))

for i in range(len(news_dates[1:]) + 1):
  new_path = str(path + str(news_dates[i]) + json_end)
  new_df = pd.read_json(new_path)
  master_df = master_df.append(new_df)
  
len(master_df)

path2 = r'/content/drive/My Drive/g20_uplevel/alton_json/'
json_end = r'.json'

master_df2 = pd.read_json(str(path2 + str(news_dates[0]) + json_end))

for i in range(len(news_dates[1:]) + 1):
  try:
    new_path = str(path2 + str(news_dates[i]) + json_end)
    new_df2 = pd.read_json(new_path)
    master_df2 = master_df2.append(new_df2)
  except:
    pass
  
len(master_df2)

path3 = r'/content/drive/My Drive/g20_uplevel/eddie_json/'
json_end = r'eddie.json'

master_df3 = pd.read_json(str(path3 + str(news_dates[0]) + json_end))

for i in range(len(news_dates[1:]) + 1):
  try:
    new_path = str(path3 + str(news_dates[i]) + json_end)
    new_df3 = pd.read_json(new_path)
    master_df3 = master_df3.append(new_df3)
  except:
    pass
  
len(master_df3)

path4 = r'/content/drive/My Drive/g20_uplevel/eddie_json/japan_summit/'
json_end = r'japan_summit.json'

master_df4 = pd.read_json(str(path4 + str(news_dates[0]) + json_end))

for i in range(len(news_dates[1:]) + 1):
  try:
    new_path = str(path4 + str(news_dates[i]) + json_end)
    new_df4 = pd.read_json(new_path)
    master_df4 = master_df4.append(new_df4)
  except:
    pass
  
len(master_df4)

merged_master = pd.concat([master_df, master_df2, master_df3, master_df4])

len(merged_master)

merged_master_unique = merged_master.drop_duplicates(subset=['title', 'structuredText', 'text'])

len(merged_master_unique)

merged_master_unique.head()

import nltk
import numpy as np
import string
import wordcloud
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib import rcParams
import random
import pandas as pd
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('popular')
nltk.download('vader_lexicon')

#the vader sentiment is the framework to label the sentiments in the article
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
sia = SIA()

# test code - to test the sentiment analysis for 1 article
passage1 = merged_master_unique.iloc[1]['text']
#def sentiment_scores(passage):
  #score = sia.polarity_scores(passage)
  #print (score)

#sentiment_scores(passage1)

#building an empty list to put in all the scores
#positive scores
pos = []
#negative scores
neg = []
#neutral scores
neu = []
#compound scores - the only one we really care about
com = []

for i in range(len(merged_master_unique['text'])):
  score = sia.polarity_scores(merged_master_unique.iloc[i]['text'])
  pos.append(score['pos'])
  neg.append(score['neg'])
  neu.append(score['neu'])
  com.append(score['compound'])

#just making sure that the com has the right number of items
len(com)

#testing the dataframe method
merged_master_unique.iloc[0]['discoverDate']

#testing the string method
string1 = '2019-05-29T14:52:19.083+0000'
string1 = (string1)[:10]

#checking the modified string
string1

##creating a new list of actual dates
new_dates = []

#creating a list of new dates with the first 10 characters - the actual calendar date - iterating through all the discoverDate
for i in range(len(merged_master_unique['discoverDate'])):
  date = (merged_master_unique.iloc[i]['discoverDate'])[:10]
  new_dates.append(date)

#checking that all the dates are done
len (new_dates)

#setting the new dates as datetime objects
from datetime import datetime
[datetime.strptime(x, '%Y-%m-%d') for x in new_dates]

#checking that nothing has changed
len(new_dates)

#adding dates to a new column in the dataframe
merged_master_unique['actual_dates'] = new_dates

#checking the modifed dataframe
merged_master_unique.head()

#adding the sentiment compound score into the dataframe
merged_master_unique['compound_score'] = com

#renaming the dataframe
sentiment_df=merged_master_unique

#checking the dataframe
sentiment_df.head()

from google.colab import files

#this calculates the mean(average) compound score by date
sentiment_df.groupby('actual_dates').mean()[['compound_score']]

# just relabelling the dataframe
plot_df = sentiment_df.groupby('actual_dates').mean()[['compound_score']]

#just checking what's in the new dataframe
plot_df['compound_score']

#resetting and relabelling the dataframe to prepare for plotting. reset_index makes sure that actual_dates and compound_score are now two distinct columns
df1 = plot_df.groupby('actual_dates').sum().reset_index()

#plotting actual_dates (x-axis) by compound_score (y-axis) 
import matplotlib.pyplot as plt 

plt.xticks(rotation=90)
plt.plot(df1['actual_dates'], df1['compound_score'])

countryname = []
hostname = []
name = []
domainname = []

for i in range(len(sentiment_df)):
  placeholder = sentiment_df.iloc[i]['website']
  if type(placeholder) == dict:
    countryname.append(placeholder['countryName'])
    hostname.append(placeholder['hostName'])
    name.append(placeholder['name'])
    domainname.append(placeholder['domainName'])
  else:
    countryname.append(None)
    hostname.append(None)
    name.append(None)
    domainname.append(None)
    
#make new columns in sentiment_df dataframe
sentiment_df['countryName'] = countryname
sentiment_df['hostName'] = hostname
sentiment_df['name'] = name
sentiment_df['domainName'] = domainname

sentiment_df.head()

#just relabelling the dataframe
country_df = sentiment_df.groupby('countryName').mean()[['compound_score']]

country_df1 = country_df.groupby('countryName').sum().reset_index()

country_df1

import matplotlib.pyplot as plt 

plt.xticks(rotation=90)
plt.plot(country_df1['countryName'], country_df1['compound_score'])

sentiment_df.head()

news_dates

#defining the dates week by week
week_1 = news_dates[0:7]
week_2 = news_dates[7:14]
week_3 = news_dates[14:21]
week_4 = news_dates[21:28]
week_5 = news_dates[28:35]
week_6 = news_dates[35:42]
week_7 = news_dates[42:49]
week_8 = news_dates[49:56]
week_9 = ['2019-07-24', '2019-07-25', '2019-07-26', '2019-07-27', '2019-07-28', '2019-07-29']

#checking what's in the various lists
print (week_1)
print (week_2)
print (week_3)
print (week_4)
print (week_5)
print (week_6)
print (week_7)
print (week_8)
print (week_9)

dates = sentiment_df['actual_dates']

len(dates)

#going through the dates in actual_dates in the df

weeks = []

for date in dates:
  for i in week_1:
    if date == i:
      weeks.append(1)
    else:
      pass

len(weeks)

weeks_2 = []

for date in dates:
  for i in week_2:
    if date == i:
      weeks_2.append(2)
    else:
      pass

len (weeks_2)

weeks_3 = []

for date in dates:
  for i in week_3:
    if date == i:
      weeks_3.append(3)
    else:
      pass

len (weeks_3)

weeks_4 = []

for date in dates:
  for i in week_4:
    if date == i:
      weeks_4.append(4)
    else:
      pass

len (weeks_4)

weeks_5 = []

for date in dates:
  for i in week_5:
    if date == i:
      weeks_5.append(5)
    else:
      pass

len (weeks_5)

weeks_6 = []

for date in dates:
  for i in week_6:
    if date == i:
      weeks_6.append(6)
    else:
      pass

len (weeks_6)

weeks_7 = []

for date in dates:
  for i in week_7:
    if date == i:
      weeks_7.append(7)
    else:
      pass

len (weeks_7)

weeks_8 = []

for date in dates:
  for i in week_8:
    if date == i:
      weeks_8.append(8)
    else:
      pass

len (weeks_8)

weeks_9 = []

for date in dates:
  for i in week_9:
    if date == i:
      weeks_9.append(9)
    else:
      pass

len (weeks_9)

#checking that the total number of values is correct
actual_week = weeks + weeks_2 + weeks_3 + weeks_4 + weeks_5 + weeks_6 + weeks_7 + weeks_8 + weeks_9

len(actual_week)

#checking that the weeks had been attached to the maind dataframe
sentiment_df['actual_week'] = actual_week

sentiment_df.head()

#generate weekly average sentiment score
weeks_score_df = sentiment_df.groupby('actual_week').mean()[['compound_score']]

week_score = weeks_score_df['compound_score']

week_score

#making sure that there are 9 long texts - one for each week
weekly_text_list = []

for d in sentiment_df.groupby(['actual_week'])['text']:
  weekly_df = ' '.join(d[1])
  weekly_text_list.append(weekly_df)

len (weekly_text_list)

#running the topic model code



from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.feature_extraction.text import CountVectorizer

import nltk
nltk.download('punkt')

def split_text_into_paras(text):
  sentences_per_para = 10
  sentences_list = nltk.tokenize.sent_tokenize(text)
  para_list = []
  new_para = ''
  sent_count_per_para = 0
  
  for sentence in sentences_list:
    if (sent_count_per_para < sentences_per_para):
      new_para+=sentence
      sent_count_per_para+=1
    elif(sent_count_per_para==sentences_per_para):
      para_list.append(new_para)
      new_para = ''
      sent_count_per_para = 0
  para_list.append(new_para)
  return para_list




def display_topics(model, feature_names, no_top_word):
  top_20_topics = []
  word_list = []
  for topic_idx, topic in enumerate(model.components_):
    word_list.append(([feature_names[i] for i in topic.argsort()[:-no_top_word - 1:-1]]))
  return (word_list)
    
#declaring stopwords variable, and then adding more stopwords into the existing list
stopwords = nltk.corpus.stopwords.words('english')
stopwords.extend(['said', 'g20', 'summit', 'also', '20', 'G20', 'Summit', 'per', 'cent'])


#generating the topics for all the 9 weeks - long list of 90 topics 

all_list = []
for i in range(len(weekly_text_list)):
  words = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stopwords)
  bag_of_words = words.fit_transform(split_text_into_paras(weekly_text_list[i]))
  word_names = words.get_feature_names()
  lda = LDA(n_components = 10).fit(bag_of_words)
  all_list.append(([i] + display_topics(lda, word_names, 5)))

len (all_list)

for i in range(len(weekly_text_list)):
  words = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stopwords)
  bag_of_words = words.fit_transform(split_text_into_paras(weekly_text_list[i]))
  word_names = words.get_feature_names()
  lda = LDA(n_components = 10).fit(bag_of_words)
  print (display_topics(lda, word_names, 5))

#should really think about how to make this less manual - how to declare every iterate output as a variable

week_1=[['china', 'trade', 'chinese', 'us', 'trump'], ['india', 'us', 'trade', 'turkey', 'data'], ['trade', 'finance', 'meeting', 'global', 'japan'], ['tax', 'countries', 'digital', 'finance', 'companies'], ['japan', 'trade', 'crypto', 'year', 'yields'], ['climate', 'change', 'global', 'energy', 'new'], ['trade', 'oil', 'crude', 'barrel', 'thursday'], ['year', 'trade', 'percent', 'global', 'growth'], ['trump', 'president', 'minister', 'mr', 'one'], ['financial', 'market', 'china', 'canada', 'economic']]
week_2=[['market', 'fed', 'data', 'rate', 'meeting'], ['global', 'trade', 'financial', 'countries', 'japan'], ['trump', 'china', 'trade', 'tariffs', 'chinese'], ['energy', 'plastic', 'putin', 'russia', 'japan'], ['china', 'trump', 'trade', 'chinese', 'two'], ['year', 'new', 'tax', 'companies', 'india'], ['trade', 'growth', 'meeting', 'global', 'china'], ['iran', 'oil', 'energy', 'attacks', 'united'], ['hong', 'kong', 'pompeo', 'would', 'china'], ['trade', 'week', 'markets', 'cut', 'index']]
week_3=[['hong', 'kong', 'china', 'bill', 'xi'], ['market', 'week', 'us', 'investors', 'trade'], ['president', 'minister', 'india', 'prime', 'leaders'], ['xi', 'trade', 'economy', 'global', 'development'], ['trump', 'president', 'trudeau', 'canada', 'ivanka'], ['china', 'trade', 'trump', 'us', 'chinese'], ['research', 'crypto', 'fatf', 'financial', 'information'], ['north', 'kim', 'trump', 'korea', 'xi'], ['global', 'japan', 'countries', 'world', 'international'], ['week', 'dollar', 'us', 'iran', 'japan']]
week_4=[['huawei', 'us', 'companies', 'company', 'trump'], ['turkey', 'russia', 'sanctions', 'erdogan', 'trump'], ['china', 'us', 'trade', 'growth', 'global'], ['data', 'trade', 'index', 'cut', 'june'], ['climate', '2019', 'global', 'energy', 'change'], ['chinese', 'us', 'china', 'year', 'financial'], ['trade', 'us', 'india', 'tariffs', 'china'], ['china', 'trump', 'canada', 'chinese', 'xi'], ['trump', 'president', 'world', 'ivanka', 'one'], ['saudi', 'merkel', 'arabia', 'minister', 'shaking']]
week_5=[['china', 'trade', 'chinese', 'trump', 'tariffs'], ['week', 'trade', 'fed', 'last', 'index'], ['japan', 'india', 'minister', 'world', 'global'], ['percent', 'growth', 'global', 'energy', 'us'], ['year', '2019', 'million', 'company', '2018'], ['president', 'trump', 'russia', 'told', 'would'], ['china', 'canada', 'united', 'states', 'president'], ['us', 'trump', 'huawei', 'india', 'pakistan'], ['government', 'hong', 'kong', 'people', 'bill'], ['trade', 'countries', 'wto', 'world', 'system']]
week_6=[['trump', 'mueller', 'report', 'charges', 'thursday'], ['market', 'share', '2019', 'investment', 'june'], ['global', 'world', 'japan', 'climate', 'international'], ['plastic', 'iran', 'japan', 'deal', 'would'], ['north', 'korea', 'trump', 'japan', 'us'], ['north', 'canada', 'canadian', 'trudeau', 'trump'], ['people', 'one', 'china', 'like', 'us'], ['china', 'trade', 'chinese', 'trump', 'huawei'], ['trade', 'us', 'xi', 'trump', 'two'], ['trump', 'president', 'merkel', 'minister', 'prime']]
week_7=[['china', 'trade', 'asean', 'us', 'market'], ['china', 'hong', 'kong', 'would', 'united'], ['week', 'bloomberg', 'stocks', 'trade', 'market'], ['trump', 'president', 'meeting', 'japan', 'putin'], ['trump', 'us', 'japan', 'president', 'would'], ['india', 'trade', 'new', 'year', 'us'], ['trade', 'trump', 'chinese', 'tariffs', 'china'], ['japan', 'plastic', 'abe', 'iran', 'japanese'], ['trump', 'president', 'japan', 'press', 'south'], ['north', 'korea', 'kim', 'xi', 'trump']]
week_8=[['2019', 'business', 'company', '30', 'technology'], ['china', 'trade', 'trump', 'chinese', 'us'], ['telescope', 'construction', 'hawaii', 'mauna', 'kea'], ['north', 'korea', 'kim', 'trump', 'nuclear'], ['trump', 'economy', 'growth', 'rates', 'president'], ['trump', 'president', 'european', 'eu', 'iran'], ['south', 'korea', 'remains', 'trump', 'japan'], ['trump', 'va', 'veterans', 'care', 'years'], ['us', 'one', 'year', 'would', 'people'], ['world', 'new', 'voice', 'first', 'government']]
week_9=[['rating', 'company', 'research', 'average', 'stock'], ['mountain', 'telescope', 'time', 'hawaii', 'one'], ['new', 'climate', 'government', 'people', 'need'], ['open', '2020', 'event', 'wwrs', 'worldwide'], ['shares', 'quarter', 'stock', '000', 'company'], ['south', 'japan', 'japanese', 'work', 'medical'], ['asean', 'africa', 'president', 'women', 'event'], ['digital', 'business', '2019', 'technology', 'marketing'], ['trump', 'north', 'korea', 'china', 'trade'], ['company', 'products', 'time', 'work', 'niagara']]

#create columns, topic by topic - should figure out how to do this less manually
topic_1 = [week_1[0], week_2[0], week_3[0], week_4[0], week_5[0], week_6[0], week_7[0], week_8[0], week_9[0]]
topic_2 = [week_1[1], week_2[1], week_3[1], week_4[1], week_5[1], week_6[1], week_7[1], week_8[1], week_9[1]]
topic_3= [week_1[2], week_2[2], week_3[2], week_4[2], week_5[2], week_6[2], week_7[2], week_8[2], week_9[2]]
topic_4= [week_1[3], week_2[3], week_3[3], week_4[3], week_5[3], week_6[3], week_7[3], week_8[3], week_9[3]]
topic_5= [week_1[4], week_2[4], week_3[4], week_4[4], week_5[4], week_6[4], week_7[4], week_8[4], week_9[4]]
topic_6= [week_1[5], week_2[5], week_3[5], week_4[5], week_5[5], week_6[5], week_7[5], week_8[5], week_9[5]]
topic_7= [week_1[6], week_2[6], week_3[6], week_4[6], week_5[6], week_6[6], week_7[6], week_8[6], week_9[6]]
topic_8= [week_1[7], week_2[7], week_3[7], week_4[7], week_5[7], week_6[7], week_7[7], week_8[7], week_9[7]]
topic_9= [week_1[8], week_2[8], week_3[8], week_4[8], week_5[8], week_6[8], week_7[8], week_8[8], week_9[8]]
topic_10= [week_1[9], week_2[9], week_3[9], week_4[9], week_5[9], week_6[9], week_7[9], week_8[9], week_9[9]]

import pandas as pd

#declare first column of weeks
week = ['week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'week_9']

topic_df = pd.DataFrame()
topic_df['week'] = week
topic_df['topic 1'] = topic_1
topic_df['topic 2'] = topic_2
topic_df['topic 3'] = topic_3
topic_df['topic 4'] = topic_4
topic_df['topic 5'] = topic_5
topic_df['topic 6'] = topic_6
topic_df['topic 7'] = topic_7
topic_df['topic 8'] = topic_8
topic_df['topic 9'] = topic_9
topic_df['topic 10'] = topic_10

topic_df

#add additional sentiment score to the end of the dataframe

topic_df['Sentiment_score'] =  [0.125433, 0.205268, 0.214619, 0.216383, 0.254744, 0.144798, 0.352770, 0.454552, 0.587801]

topic_df

import numpy as np
from os import path
from PIL import Image
from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS

stopwords = set(STOPWORDS)
stopwords.update(['said', 'g20', 'summit', 'also', '20', 'G20', 'Summit', 'per', 'cent', 'will'])

for i in range(len(weekly_text_list)):
  wordcloud_ = WordCloud(stopwords=stopwords, background_color ='white').generate(weekly_text_list[i])
  plt.figure(figsize=(20, 10))
  plt.imshow(wordcloud_, interpolation='bilinear')
  plt.axis("off")
  plt.show()

sources = []
for i in sentiment_df['url']:
  if i is not None:
    try:
      split_url = i.split(".")
      source= split_url[1]
      sources.append(source)
    except:
      sources.append(None)
  else:
    sources.append(None)
    
sentiment_df['sources'] = sources

sentiment_df.head()

sentiment_df['sources'].value_counts()

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE Australia, else rejected
is_australia =  sentiment_df['countryName']=='Australia'

#applies the filter to the dataframe
sentiment_australia = sentiment_df[is_australia]

len(sentiment_australia)

aus_wks_score = sentiment_australia.groupby('actual_week').mean()['compound_score']
                                                                     
country_wks_score = pd.DataFrame()

country_wks_scoreweek = ['week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'week_9']

country_wks_score['Australia'] = aus_wks_score

country_wks_score

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE America, else rejected
is_america_US =  sentiment_df['countryName']=='United States'

#applies the filter to the dataframe
sentiment_america = sentiment_df[is_america_US]

sentiment_america.head()

len(sentiment_america)

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE Australia, else rejected
is_china =  sentiment_df['countryName']=='China'

#applies the filter to the dataframe
sentiment_china = sentiment_df[is_china]

sentiment_china.head()

len(sentiment_china)

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE Australia, else rejected
is_japan =  sentiment_df['countryName']=='Japan'

#applies the filter to the dataframe
sentiment_japan = sentiment_df[is_japan]

sentiment_japan.head()

len (sentiment_japan)

jpn_wks_score = sentiment_japan.groupby('actual_week').mean()['compound_score']

country_wks_score['Japan'] = jpn_wks_score

country_wks_score

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE none, else rejected
is_none =  sentiment_df['countryName']==''

#applies the filter to the dataframe
sentiment_none = sentiment_df[is_none]

sentiment_none.head()

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE Australia, else rejected
is_france =  sentiment_df['countryName']=='France'

#applies the filter to the dataframe
sentiment_france = sentiment_df[is_france]

sentiment_france.head()

len (sentiment_france)

sentiment_df['countryName'].value_counts()

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE Australia, else rejected
is_canada =  sentiment_df['countryName']=='Canada'

#applies the filter to the dataframe
sentiment_canada = sentiment_df[is_canada]

sentiment_canada.head()

cnd_wks_score = sentiment_canada.groupby('actual_week').mean()['compound_score']

country_wks_score['Canada'] = cnd_wks_score

country_wks_score

#creating dataframe subsets by country

#this creates a filter - declares that the values in the countryName column MUST BE UK, else rejected
is_uk =  sentiment_df['countryName']=='United Kingdom'

#applies the filter to the dataframe
sentiment_uk = sentiment_df[is_uk]

sentiment_uk.head()

uk_wks_score = sentiment_uk.groupby('actual_week').mean()['compound_score']

country_wks_score['UK'] = uk_wks_score

country_wks_score

is_sgp =  sentiment_df['countryName']=='Singapore'
sentiment_sgp = sentiment_df[is_sgp]

sgp_wks_score = sentiment_sgp.groupby('actual_week').mean()['compound_score']

country_wks_score['Singapore'] = sgp_wks_score

country_wks_score

is_ind =  sentiment_df['countryName']=='India'
sentiment_ind = sentiment_df[is_ind]

ind_wks_score = sentiment_ind.groupby('actual_week').mean()['compound_score']

country_wks_score['India'] = ind_wks_score

country_wks_score

is_sws =  sentiment_df['countryName']=='Switzerland'
sentiment_sws = sentiment_df[is_sws]

sws_wks_score = sentiment_sws.groupby('actual_week').mean()['compound_score']

country_wks_score['Switzerland'] = sws_wks_score

country_wks_score

cntry_wks_score = country_wks_score.groupby('actual_week').sum().reset_index()

cntry_wks_score

sentiment_df['sources'].value_counts()

is_sws =  sentiment_df['countryName']=='Switzerland'
sentiment_sws = sentiment_df[is_sws]

sws_wks_score = sentiment_sws.groupby('actual_week').mean()['compound_score']

country_wks_score['Switzerland'] = sws_wks_score

country_wks_score

is_mktscnr = sentiment_df['sources']=='marketscreener'
sentiment_mktscnr  = sentiment_df[is_mktscnr]

mktscnr_wks_score = sentiment_mktscnr.groupby('actual_week').mean()['compound_score']

source_wks_score = pd.DataFrame()

source_wks_score['Market Screener'] = mktscnr_wks_score

source_wks_score

is_cna = sentiment_df['sources']=='channelnewsasia'
sentiment_cna = sentiment_df[is_cna]

cna_wks_score = sentiment_cna.groupby('actual_week').mean()['compound_score']

source_wks_score['Channel News Asia'] = cna_wks_score

source_wks_score

is_fex = sentiment_df['sources']=='financialexpress'
sentiment_fex = sentiment_df[is_fex]

fex_wks_score = sentiment_fex.groupby('actual_week').mean()['compound_score']

source_wks_score['Financial Express'] = fex_wks_score

source_wks_score

is_jpnt = sentiment_df['sources']=='japantimes'
sentiment_jpnt = sentiment_df[is_jpnt]

jpnt_wks_score = sentiment_jpnt.groupby('actual_week').mean()['compound_score']

source_wks_score['Japan Times'] = jpnt_wks_score

source_wks_score

is_bizindr = sentiment_df['sources']=='businessinsider'
sentiment_bizindr = sentiment_df[is_bizindr]

bizindr_wks_score = sentiment_bizindr.groupby('actual_week').mean()['compound_score']


source_wks_score['Business Insider'] = bizindr_wks_score

source_wks_score

is_reuters = sentiment_df['sources']=='reuters'
sentiment_reuters = sentiment_df[is_reuters]

reuters_wks_score = sentiment_reuters.groupby('actual_week').mean()['compound_score']


source_wks_score['Reuters'] = reuters_wks_score

reuters_wks_score

source_wks_score.groupby('actual_week').sum().reset_index()

